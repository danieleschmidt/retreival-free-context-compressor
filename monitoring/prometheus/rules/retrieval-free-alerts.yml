groups:
  - name: retrieval-free.rules
    rules:
      # Application Health Alerts
      - alert: RetrievalFreeServiceDown
        expr: up{job="retrieval-free-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Retrieval-Free Context Compressor service is down"
          description: "The Retrieval-Free Context Compressor service has been down for more than 1 minute."

      - alert: RetrievalFreeHighErrorRate
        expr: rate(http_requests_total{job="retrieval-free-app",status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second for the last 5 minutes."

      # Performance Alerts
      - alert: RetrievalFreeHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="retrieval-free-app"}[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s for the last 5 minutes."

      - alert: RetrievalFreeCompressionSlowdown
        expr: histogram_quantile(0.95, rate(compression_duration_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Compression process is slow"
          description: "95th percentile compression time is {{ $value }}s, which is above the 5s threshold."

      # Resource Alerts
      - alert: RetrievalFreeHighMemoryUsage
        expr: process_resident_memory_bytes{job="retrieval-free-app"} / 1024 / 1024 > 2048
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}MB, which is above the 2GB threshold."

      - alert: RetrievalFreeHighCPUUsage
        expr: rate(process_cpu_seconds_total{job="retrieval-free-app"}[5m]) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}% for the last 10 minutes."

      # GPU Alerts (if available)
      - alert: RetrievalFreeGPUMemoryHigh
        expr: nvidia_ml_py_memory_used_bytes / nvidia_ml_py_memory_total_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU memory usage is high"
          description: "GPU memory usage is {{ $value | humanizePercentage }} which is above 90%."

      - alert: RetrievalFreeGPUUtilizationLow
        expr: nvidia_ml_py_utilization_gpu < 10
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "GPU utilization is low"
          description: "GPU utilization has been below 10% for 30 minutes. Consider scaling down GPU resources."

      # Model and Data Alerts
      - alert: RetrievalFreeModelLoadFailure
        expr: increase(model_load_failures_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Model loading failures detected"
          description: "{{ $value }} model load failures in the last 5 minutes."

      - alert: RetrievalFreeCompressionQualityDegradation
        expr: compression_quality_score < 0.8
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Compression quality degradation"
          description: "Compression quality score is {{ $value }}, which is below the 0.8 threshold."

      # Storage Alerts
      - alert: RetrievalFreeDiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/app"} / node_filesystem_size_bytes{mountpoint="/app"}) < 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space"
          description: "Available disk space is {{ $value | humanizePercentage }} which is below 10%."

      - alert: RetrievalFreeCacheGrowth
        expr: increase(cache_size_bytes[1h]) > 1073741824  # 1GB
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "Cache is growing rapidly"
          description: "Cache has grown by {{ $value | humanizeBytes }} in the last hour."

  - name: infrastructure.rules
    rules:
      # System Resource Alerts
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90% for 5 minutes."

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 90% for 10 minutes."

      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Disk space is below 10% on {{ $labels.device }}."

      - alert: ContainerKilled
        expr: time() - container_last_seen > 60
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Container killed"
          description: "Container {{ $labels.name }} has been killed."

      - alert: ContainerHighMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container high memory usage"
          description: "Container {{ $labels.name }} memory usage is above 90%."

  - name: monitoring.rules
    rules:
      # Monitoring Stack Health
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus target down"
          description: "Target {{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute."

      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus configuration reload failure"
          description: "Prometheus configuration reload error."

      - alert: AlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "AlertManager configuration reload failure"
          description: "AlertManager configuration reload error."

      - alert: GrafanaServiceDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Grafana service is down"
          description: "Grafana has been down for more than 1 minute."